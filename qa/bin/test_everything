#!/usr/bin/env python3
"""
ExaBGP Complete Test Suite Runner

Runs all required tests in sequence, exits on first failure.
Use this instead of running individual test commands.

Usage:
  ./qa/bin/test_everything           # Run all tests with progress
  ./qa/bin/test_everything -l        # List available tests
  ./qa/bin/test_everything --skip-mypy  # Skip mypy type checking
  ./qa/bin/test_everything unit      # Run single test by name
  ./qa/bin/test_everything 3         # Run single test by number
  ./qa/bin/test_everything unit encoding  # Run multiple tests

Exit codes:
  0 - All tests passed
  1 - Test failed
  2 - Invalid arguments
"""

import argparse
import json
import os
import re
import subprocess
import sys
import time
from pathlib import Path


class Colors:
    """ANSI color codes"""

    GREEN = '\033[92m'
    RED = '\033[91m'
    YELLOW = '\033[93m'
    CYAN = '\033[96m'
    BLUE = '\033[94m'
    MAGENTA = '\033[95m'
    WHITE = '\033[97m'
    BOLD = '\033[1m'
    DIM = '\033[2m'
    RESET = '\033[0m'

    @classmethod
    def disable(cls):
        """Disable colors for non-TTY output"""
        cls.GREEN = ''
        cls.RED = ''
        cls.YELLOW = ''
        cls.CYAN = ''
        cls.BLUE = ''
        cls.MAGENTA = ''
        cls.WHITE = ''
        cls.BOLD = ''
        cls.DIM = ''
        cls.RESET = ''


# Disable colors if not in a TTY
if not sys.stdout.isatty():
    Colors.disable()


# Default expected times (seconds) for first run
DEFAULT_TIMES: dict[str, int] = {
    'ruff-format': 3,
    'ruff-check': 3,
    'mypy': 60,
    'unit': 15,
    'config': 2,
    'no-neighbor': 5,
    'encode-decode': 3,
    'parsing': 6,
    'json': 3,
    'api-encode': 2,
    'cmd-roundtrip': 5,
    'decoding': 10,
    'encoding': 45,
    'api': 90,
    'cli': 10,
    'type-ignore': 1,
}

CACHE_FILE = Path.home() / '.cache' / 'exabgp' / 'test_times.json'
MAX_HISTORY = 5


def load_timing_cache() -> dict[str, list[float]]:
    """Load timing history from cache file"""
    if CACHE_FILE.exists():
        try:
            with open(CACHE_FILE) as f:
                return json.load(f)
        except (json.JSONDecodeError, OSError):
            pass
    return {}


def save_timing_cache(cache: dict[str, list[float]]) -> None:
    """Save timing history to cache file"""
    CACHE_FILE.parent.mkdir(parents=True, exist_ok=True)
    try:
        with open(CACHE_FILE, 'w') as f:
            json.dump(cache, f, indent=2)
    except OSError:
        pass  # Non-critical, ignore write failures


def get_expected_time(cache: dict[str, list[float]], test_name: str) -> float:
    """Get expected time from cache or default"""
    if test_name in cache and cache[test_name]:
        return sum(cache[test_name]) / len(cache[test_name])
    return float(DEFAULT_TIMES.get(test_name, 10))


def record_time(cache: dict[str, list[float]], test_name: str, elapsed: float) -> None:
    """Record actual time to cache (keep last N)"""
    if test_name not in cache:
        cache[test_name] = []
    cache[test_name].append(elapsed)
    cache[test_name] = cache[test_name][-MAX_HISTORY:]


def format_time(seconds: float) -> str:
    """Format seconds as human-readable string"""
    if seconds < 60:
        return f'{seconds:.1f}s'
    minutes = int(seconds // 60)
    secs = seconds % 60
    return f'{minutes}m{secs:.0f}s'


def run_command(cmd: str, env: dict | None = None, shell: bool = True) -> tuple[int, str, str]:
    """Run a command and return exit code, stdout, stderr"""
    full_env = os.environ.copy()
    if env:
        full_env.update(env)

    process = subprocess.Popen(cmd, shell=shell, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=full_env, text=True)
    stdout, stderr = process.communicate()
    return process.returncode, stdout, stderr


# Test definitions: (name, command, description)
# Commands can be a string or list of strings (run sequentially)
TEST_COMMANDS: dict[str, str | list[str]] = {
    'ruff-format': 'uv run ruff format src tests',
    'ruff-check': 'uv run ruff check src qa tests',
    'mypy': 'uv run mypy src/exabgp/',
    'unit': 'env exabgp_log_enable=false uv run pytest ./tests/unit/ ./tests/fuzz/ -q',
    'config': './sbin/exabgp configuration validate -nrv ./etc/exabgp/conf-ipself6.conf',
    'no-neighbor': './qa/scripts/test_no_neighbor.py',
    'encode-decode': './qa/scripts/test_encode_decode.py',
    'parsing': './qa/bin/test_parsing',
    'json': './qa/bin/test_json',
    'api-encode': './qa/bin/test_api_encode',
    'cmd-roundtrip': './qa/bin/test_api_encode --self-check',
    'decoding': './qa/bin/functional decoding',
    'encoding': ['killall -9 python 2>/dev/null; sleep 0.5', './qa/bin/functional encoding --quiet'],
    'api': './qa/bin/functional api --quiet',
    'cli': './qa/bin/functional cli --quiet',
    'type-ignore': './qa/bin/check_type_ignores',
}

TEST_DESCRIPTIONS: dict[str, str] = {
    'ruff-format': 'Ruff format check',
    'ruff-check': 'Ruff lint check',
    'mypy': 'Mypy strict type check',
    'unit': 'Unit tests (pytest)',
    'config': 'Configuration validation',
    'no-neighbor': 'No-neighbor API test',
    'encode-decode': 'Encode/decode CLI test',
    'parsing': 'Config file validation',
    'json': 'JSON decode regression tests',
    'api-encode': 'API command encode tests',
    'cmd-roundtrip': 'API cmd round-trip (raw→cmd→raw)',
    'decoding': 'Functional decoding tests',
    'encoding': 'Functional encoding tests',
    'api': 'Functional API tests',
    'cli': 'Functional CLI tests',
    'type-ignore': 'Type ignore regression check',
}

# Ordered list of test names
TEST_ORDER = [
    'ruff-format',
    'ruff-check',
    'mypy',
    'unit',
    'config',
    'no-neighbor',
    'encode-decode',
    'parsing',
    'json',
    'api-encode',
    'cmd-roundtrip',
    'decoding',
    'encoding',
    'cli',
    'api',
    'type-ignore',
]


def run_test(name: str, verbose: bool = False) -> tuple[bool, str, str]:
    """Run a test by name, return (success, stdout, stderr)"""
    cmd = TEST_COMMANDS[name]

    if isinstance(cmd, list):
        # Multiple commands - run sequentially
        all_stdout = []
        all_stderr = []
        for c in cmd:
            if verbose:
                print(f'{Colors.DIM}  $ {c}{Colors.RESET}')
            returncode, stdout, stderr = run_command(c)
            all_stdout.append(stdout)
            all_stderr.append(stderr)
            # Don't fail on setup commands (like killall)
            if returncode != 0 and c == cmd[-1]:
                return False, '\n'.join(all_stdout), '\n'.join(all_stderr)
        stdout = '\n'.join(all_stdout)
        stderr = '\n'.join(all_stderr)
        returncode = 0
    else:
        if verbose:
            print(f'{Colors.DIM}  $ {cmd}{Colors.RESET}')
        returncode, stdout, stderr = run_command(cmd)

    # Special case: functional tests need 100% pass rate check
    if name in ('encoding', 'api', 'cli') and returncode == 0:
        pass_rate_match = re.search(r'100(?:\.\d+)?%', stdout)
        if not pass_rate_match:
            return False, stdout, stderr

    return returncode == 0, stdout, stderr


def list_tests(cache: dict[str, list[float]], verbose: bool = False) -> None:
    """Print available tests with expected times"""
    print(f'{Colors.CYAN}{"─" * 60}{Colors.RESET}')
    print(f'{Colors.BOLD}{Colors.WHITE}  Available Tests{Colors.RESET}')
    print(f'{Colors.CYAN}{"─" * 60}{Colors.RESET}')
    for i, name in enumerate(TEST_ORDER, 1):
        expected = get_expected_time(cache, name)
        desc = TEST_DESCRIPTIONS[name]
        num = f'{Colors.CYAN}{i}.{Colors.RESET}'
        test_name = f'{Colors.WHITE}{Colors.BOLD}{name:14}{Colors.RESET}'
        time_str = f'{Colors.BLUE}(~{format_time(expected):>5}){Colors.RESET}'
        desc_str = f'{Colors.DIM}{desc}{Colors.RESET}'
        print(f'  {num} {test_name} {time_str}  {desc_str}')
        if verbose:
            cmd = TEST_COMMANDS[name]
            if isinstance(cmd, list):
                for c in cmd:
                    print(f'{Colors.DIM}      $ {c}{Colors.RESET}')
            else:
                print(f'{Colors.DIM}      $ {cmd}{Colors.RESET}')
    print(f'{Colors.CYAN}{"─" * 60}{Colors.RESET}')


def resolve_test_selector(selector: str) -> int | None:
    """Resolve a test selector (name or number) to index, or None if invalid"""
    # Try as number first
    try:
        num = int(selector)
        if 1 <= num <= len(TEST_ORDER):
            return num - 1
    except ValueError:
        pass

    # Try as name
    for i, name in enumerate(TEST_ORDER):
        if name == selector:
            return i

    return None


def run_tests(test_indices: list[int], cache: dict[str, list[float]], verbose: bool = False) -> int:
    """Run selected tests with progress output. Returns exit code."""
    start_time = time.time()
    total = len(test_indices)
    results: list[tuple[str, bool, float, float]] = []  # (name, success, elapsed, expected)
    cumulative_time = 0.0  # Track time spent on previous tests

    for run_num, test_idx in enumerate(test_indices, 1):
        name = TEST_ORDER[test_idx]
        desc = TEST_DESCRIPTIONS[name]
        expected = get_expected_time(cache, name)

        # Calculate expected total time at end of this test
        expected_finish = cumulative_time + expected

        # Print progress line with colors
        progress = f'{Colors.CYAN}[{run_num}/{total}]{Colors.RESET}'
        test_name = f'{Colors.WHITE}{Colors.BOLD}{name:14}{Colors.RESET}'
        eta_str = f'{Colors.DIM}({Colors.MAGENTA}ETA {format_time(expected_finish)}{Colors.DIM}){Colors.RESET}'

        if verbose:
            print(f'{progress} {test_name} {eta_str}')
        else:
            print(f'{progress} {test_name} {eta_str} ... ', end='', flush=True)

        test_start = time.time()
        success, stdout, stderr = run_test(name, verbose=verbose)
        elapsed = time.time() - test_start

        # Update cumulative time with actual elapsed
        cumulative_time += elapsed

        # Record timing and result
        record_time(cache, name, elapsed)
        results.append((name, success, elapsed, expected))

        # Show result with colored time (actual vs expected)
        elapsed_str = f'{Colors.BLUE}{format_time(elapsed)}{Colors.RESET}'
        expected_str = f'{Colors.DIM}(~{format_time(expected)}){Colors.RESET}'
        if success:
            print(f'{Colors.GREEN}✓{Colors.RESET} {elapsed_str} {expected_str}')
        else:
            print(f'{Colors.RED}✗{Colors.RESET} {elapsed_str} {expected_str}')

        if not success:
            # Show failure details
            print(f'\n{Colors.RED}{Colors.BOLD}FAILED: {desc}{Colors.RESET}')
            if stdout:
                print(stdout)
            if stderr:
                print(stderr)
            save_timing_cache(cache)
            return 1

    # All passed - show summary
    total_elapsed = time.time() - start_time
    save_timing_cache(cache)

    # Print summary table (verbose only)
    if verbose:
        total_expected = sum(exp for _, _, _, exp in results)
        print()
        print(f'{Colors.CYAN}{"─" * 60}{Colors.RESET}')
        print(f'{Colors.BOLD}{Colors.WHITE}  Test Summary{Colors.RESET}')
        print(f'{Colors.CYAN}{"─" * 60}{Colors.RESET}')
        for name, success, elapsed, expected in results:
            status = f'{Colors.GREEN}✓{Colors.RESET}' if success else f'{Colors.RED}✗{Colors.RESET}'
            time_str = f'{Colors.BLUE}{format_time(elapsed):>6}{Colors.RESET}'
            exp_str = f'{Colors.DIM}(~{format_time(expected):>5}){Colors.RESET}'
            print(f'  {status} {name:14} {time_str} {exp_str}')
        print(f'{Colors.CYAN}{"─" * 60}{Colors.RESET}')
        print(f'  {Colors.GREEN}{Colors.BOLD}Total: {format_time(total_elapsed)}{Colors.RESET} {Colors.DIM}(~{format_time(total_expected)}){Colors.RESET}')
        print(f'{Colors.CYAN}{"─" * 60}{Colors.RESET}')
        print()

    print(f'{Colors.GREEN}{Colors.BOLD}✓ All {total} tests passed in {format_time(total_elapsed)}{Colors.RESET}')
    return 0


def main() -> int:
    """Main entry point"""
    parser = argparse.ArgumentParser(
        description='ExaBGP test suite runner',
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument('-l', '--list', action='store_true', help='List available tests')
    parser.add_argument('-v', '--verbose', action='store_true', help='Show commands being run')
    parser.add_argument('--skip-mypy', action='store_true', help='Skip mypy type checking')
    parser.add_argument('tests', nargs='*', help='Tests to run (name or number)')

    args = parser.parse_args()
    cache = load_timing_cache()

    if args.list:
        list_tests(cache, verbose=args.verbose)
        return 0

    # Determine which tests to run
    if args.tests:
        test_indices = []
        for selector in args.tests:
            idx = resolve_test_selector(selector)
            if idx is None:
                print(f'{Colors.RED}Unknown test: {selector}{Colors.RESET}')
                print('Use -l to list available tests')
                return 2
            test_indices.append(idx)
    else:
        # Run all tests
        test_indices = list(range(len(TEST_ORDER)))

    # Apply --skip-mypy filter
    if args.skip_mypy:
        mypy_idx = TEST_ORDER.index('mypy') if 'mypy' in TEST_ORDER else None
        if mypy_idx is not None:
            test_indices = [i for i in test_indices if i != mypy_idx]

    return run_tests(test_indices, cache, verbose=args.verbose)


if __name__ == '__main__':
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        print(f'\n{Colors.YELLOW}Interrupted{Colors.RESET}')
        sys.exit(130)
    except Exception as e:
        print(f'\n{Colors.RED}Error: {e}{Colors.RESET}')
        sys.exit(255)
